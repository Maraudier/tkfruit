# import packages
#%tensorflow_version 2.x
import tensorflow
from keras.models import Sequential
from keras.layers import Dropout
from keras.layers.convolutional import Conv2D
from keras.layers.convolutional import MaxPooling2D
from keras.layers.convolutional import AveragePooling2D
from keras.layers.core import Activation
from keras.layers.core import Flatten
from keras.layers.core import Dense
from keras.losses import sparse_categorical_crossentropy as SCC
from keras.utils import np_utils
from tensorflow.python.keras import backend as K
import keras.optimizers as op
from keras_preprocessing.image import ImageDataGenerator as IDG

import numpy as np

class LeNet:
  @staticmethod
  def build(lenet_version, lenet_functions, height, width, filters_first, filters_second, activation_hidden, activation_output, depth, num_labels):

    # initialize model according to image size
    model = Sequential()
    shape = (height, width, depth)

    if K.image_data_format() == "channels_first":
      shape = (depth, height, width)
    
    # first layer: 
    # filters of size 5x5
    # tanh/relu
    model.add(Conv2D(filters_first, 5, padding="same", input_shape=shape))
    model.add(Activation(activation_hidden))
    
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))

    # older implementations of lenet utilize average pooling
    # modern implementations utilize max pooling
    # size 2x2 with stride of 2
    '''if(lenet_functions == 'original'):
        model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    else:
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))'''

    # second layer: 
    # filters of size 5x5
    # tanh/relu
    model.add(Conv2D(filters_second, 5, padding="same"))
    model.add(Activation(activation_hidden))
    
    model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))

    # size 2x2 with stride of 2
    '''if(lenet_functions == 'original'):
        model.add(AveragePooling2D(pool_size=(2, 2), strides=(2, 2)))
    else:
        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))'''
        
    # flatten to one dimensional column vector
    model.add(Flatten())

    #if(lenet_version == 'one'):
    #  model.add(Dropout(0.5))
    
    # lenet versions four and five both utilized this additional fully connected layer
    # sigmoid/softmax
    if(lenet_version == 'four' or 'five'):
        model.add(Dense(120))
        model.add(Activation(activation_output))
    
    # lenet version five utilized this fully connected layer
    # sigmoid/softmax
    if(lenet_version == 'five'):
        model.add(Dense(84))
        model.add(Activation(activation_output))
        
    # lenet versions one, four, and five all utilized this full connected layer
    # sigmoid/softmax
    model.add(Dense(num_labels))
    model.add(Activation(activation_output))

    return model

# let user choose whether to run lenet model one, four or five


# when lenet was first implemented, the standard was to use tanh activation for hidden layers and sigmoid activation for output layer
# it is now standard to use relu activation for hidden layers and softmax activation for output layers
#input_functions = input("Input either "'original'" or "'modern'" to specify the activation and pooling functions of Lenet model: ")

# dictionary that corresponds to model version
# index 0: height, 1: width, 2: number of filters in first convolutional layer, 3: numbers of filters in second convolutional layer
dict_version = {'one': (28, 28, 4, 8),
                'four': (32, 32, 4, 16),
                'five': (32, 32, 6 ,16)}

# dictionary that corresponds to modern vs original standards
dict_functions = {'original': ('tanh', 'sigmoid'),
                  'modern': ('relu', 'softmax')}

# transfer dictionary contents to tuple

# preprocess data
datagen = IDG(rescale=1. / 255,
    rotation_range=20, 
	fill_mode='nearest',
    vertical_flip= True,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True)

for version in dict_version:
  tuple_version = dict_version[version]

  for functions in dict_functions:
    
    tuple_functions = dict_functions[functions]
    
    model = LeNet.build(version, 
                    functions, 
                    tuple_version[0], 
                    tuple_version[1], 
                    tuple_version[2], 
                    tuple_version[3], 
                    tuple_functions[0], 
                    tuple_functions[1], 
                    3, 6)
    
    print('Analyzing version: ' + version + ' with functions: ' + functions + '...')


    if(version == 'one' and functions == 'original'):
      train_gen = datagen.flow_from_directory(train_dir, 
	target_size=(tuple_version[0], tuple_version[1]), 
	color_mode='rgb',
	batch_size=2,
	class_mode="sparse",
	shuffle=True)
      
      test_gen = datagen.flow_from_directory(test_dir,
    target_size=(tuple_version[0], tuple_version[1]),
    color_mode='rgb',
    batch_size=2,
    class_mode='sparse',
    shuffle=True)
      
    elif(version == 'five' and functions == 'original'):
      train_gen = datagen.flow_from_directory(train_dir, 
	target_size=(tuple_version[0], tuple_version[1]), 
	color_mode='rgb',
	batch_size=2,
	class_mode="sparse",
	shuffle=True)
      
      test_gen = datagen.flow_from_directory(test_dir,
    target_size=(tuple_version[0], tuple_version[1]),
    color_mode='rgb',
    batch_size=2,
    class_mode='sparse',
    shuffle=True)
      
    learning_rate = 0.00075
    epochs = 150
    mom = .9
    nest = True
    
    opt = op.SGD(lr=learning_rate,momentum=mom, nesterov=nest)
    #opt = op.Adam(lr=.01, beta_1=.9, beta_2=.999, epsilon=1e-08, decay=0.0)

    #opt = op.SGD(lr=learning_rate)

# compile model
    model.compile(loss=SCC, optimizer=opt,
	    metrics=["sparse_categorical_accuracy"])

# fit model
    history = model.fit_generator(train_gen, epochs=epochs,
        validation_data=test_gen)
    
    print(history.history.keys())

    from keras.utils import plot_model
    save = version + '.png'
    plot_model(model, show_shapes=True, expand_nested=True, to_file=save)

    from IPython.display import SVG
    from keras.utils import model_to_dot
    
    SVG(model_to_dot(model).create(prog='dot', format='svg'))

    import matplotlib.pyplot as plt

    #history = model.fit(x, y, validation_split=0.25, epochs=50, batch_size=32, verbose=1)
#   Plot training & validation accuracy values
    plt.plot(history.history['sparse_categorical_accuracy'])
    plt.plot(history.history['val_sparse_categorical_accuracy'])
    plt.title('model accuracy')
    plt.ylabel('sparse_categorical_accuracy')
    plt.xlabel('epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()

    model_save = version + functions + '.h5'
    #path = F"/content/gdrive/My Drive/{model_save}" 
    #torch.save(model, path)
    model.save(model_save)
